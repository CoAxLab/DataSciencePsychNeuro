{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to basic classification\n",
    "\n",
    "Acts:\n",
    "- Logistic regression\n",
    "- LDA\n",
    "- QDA\n",
    "\n",
    "The lecture draws from Chapter 4 of James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). \"An introduction to statistical learning: with applications in r.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**: a categorical response variable - recall accuracy, disease diagnosis, etc\n",
    "\n",
    "**Question**: Why not linear regression?\n",
    "- if you code categories with numbers, e.g. 1, 2, 3, you *could* run a linear regression, but the results will be uninterpretable - there is no particular reason why one category is assigned the number 3, rather than 2, but linear regression assumes that 3 is not only more than 2, but that the difference between 2 and 3 is the same as that between 1 and 2. For example, if you are trying to determine which disease is consistent with a set of symptoms it does not make sense to say that disease A is more than disease B\n",
    "- for a binary case, for example, correct vs incorrect recall, you could code corrects as 1 and incorrect as 0 and run a linear regression to estimate the probability of recall. However, your linear model, as in the figure below might predict probability less than 0 or more than 1, which is impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear regression on binary data](imgs/L12Fig1a.png)\n",
    "\n",
    "**[*Note*:** since ANOVA is just a special case of linear regression, it is also inappropriate for analyzing proportion data, although this is a very common practice]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Logistic Regression model \n",
    "*(supervised method, parametric, classification)*\n",
    "\n",
    "**Assumptions**\n",
    "1.\tLinearity. Explanatory or predictor variables have a linear relationship *with the logit transformation of the dependent variable*.\n",
    "2.\tIndependence of errors. \n",
    "3.\tNo multicollinearity. \n",
    "4.\tThe response variable is dichotomous. \n",
    "\n",
    "Let's focus on the binary case, because it is the most common one. Consider the stop-signal task paradigm, in which participants have to identify a stimulus with a button press, but some of the stimuli are followed by a \"stop signal\" which instructs participants to withold response. This stop signal occurs after variable intervals, and we would like to figure out the probability of succesfull stopping for a given interval. \n",
    "\n",
    "If we apply a linear regression model, as in the figure above, we do see that it generally captures the fact that stopping is more likely as the interval increases. However, it predicts negative probability for fast intervals and at maximum it predicts ~40% probability even for the slowest speeds, for which we can see that the data shows alsmost exclusively succesfull stopping. \n",
    "\n",
    "What we need in this case is a model function that would make predictions bound between 0 and 1. The most common one is the logistic or binomial function shown on the figure below:\n",
    "\n",
    "![linear regression on binary data](imgs/L12Fig1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are not modelling Y ~ f(X), as we are doing with linear regression. Rather, we are estimating the probability of Y being a specific category, depending on the value of x or:\n",
    "\n",
    "$$P(Y = 1  |  X)$$ or $$P(Y = 1) \\sim f(X)$$ or simply $$p(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is specifically:\n",
    "$$ p(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than estimating p(X), however, we could rewrite this function it terms of the odds of Y being a specific category:\n",
    "\n",
    "$$odds(Y=1) = \\frac{p(X)}{1-p(X)} = e^{\\beta_0+\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could take the logarithm of the odds, known as the log oddds, and we get a function similar to that of linear regression.\n",
    "\n",
    "$$\\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0+\\beta_1X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio ranges from 0 to Inf, and the log odds range between -Inf and +Inf, so in this case they could be a linear function of X without the issues outlined above. \n",
    "\n",
    "---\n",
    "### Interpretation of logistic regression coefficients\n",
    "Remember, for linear regression, a beta value of 0.5 means that one unit change in X is associated with 0.5 units change in Y. For logistic regression the interpretation is not as direct - one unit change in X is a 0.5 unit change in the log odds of Y being equal to 1.\n",
    "\n",
    "There is no straightforward way to transform this into unit changes in p(X) directly. Why, after all we have a formula, right? The problem is that addition turns into multiplication in exponential space. E.g.:\n",
    "\n",
    "$$e^{\\beta_0+\\beta_1X} = e^{\\beta_0}e^{\\beta_1X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, how much p(X) will change as one predictor in X increases by one unit will depend on the values of the other predictors. You can also see that in the S-shape form of the function above. \n",
    "\n",
    "*In general, however, you can interpret the direction and relative size of coefficients the same way you do in linear regression* - positive coefficients means that p(X) increases as X increases, and bigger beta values imply a bigger increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification threshold\n",
    "\n",
    "So far, aside from the transformation on the left side of the function, the logistic model is very similar to the linear regression model. Where does classification come in? \n",
    "\n",
    "The logistic regression model makes predictions about the probability of Y being one of two categorical values. We can classify Y based on:\n",
    "\n",
    "1. Prediction of the model\n",
    "2. A threshold\n",
    "\n",
    "For example, if we chose a threshold of 0.5, then we would classify Y = 1 for any p(X) >= 0.5, and as Y = 0 for any p(X) < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimating parameters\n",
    "\n",
    "In contrast to linear regression, logistic regression does not have a closed form solution like least squares. Because logistic regression predicts *probabilities* rather than classes, we can fit it using a maximum likelihood technique. Conceptually, the goal is to find $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ such that plotting $\\hat{y}$ is close to 1 for all values where $y=1$ and close to zero for all values where $y = 0$. The basic idea behind using maximum likelihood to fit a logistic regression model is to find estimates for$\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ such that the predicted probability matches the observations as closely as possible. \n",
    "\n",
    "For example, if our outcome variable is accuracy, we would try to find the value for $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ which, when using those estimates in the model for p(X), yields a number close to one for observations corresponding to accurate trials (1), and a number close to zero for all observations that correspond to inaccurate trials (0). \n",
    "\n",
    "In the simple ($p=1$) case, the **likelihood** of $\\beta_0$ and $\\beta_1$ can be described by this function:\n",
    "\n",
    "<img src=\"imgs/likelihood_fn.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood turns the products above into sums: \n",
    "\n",
    "<img src=\"imgs/derivation_p1.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Now, to find the maximum likelihood estimates, we start by taking the derivative with respect to one component of $\\beta$. \n",
    "\n",
    "<img src=\"imgs/derivation_p2.png\" width=\"500\">\n",
    "\n",
    "While we cannot set this to 0 and find the exact solution, we can approximate the best parameter values using [numerical methods](http://www.scholarpedia.org/article/Numerical_analysis). See an explanation of how these numerical methods could be implemented [here](https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf) and [here](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiple predictors\n",
    "\n",
    "The logistic regression model is nice as it expands out to multiple predictor variables quite easily, in the same way as linear regression:\n",
    "\n",
    "$$ \\log\\bigg(\\frac{p(X)}{1-p(X)}\\bigg) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1+e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "- y = probability of getting the flu\n",
    "- x1 = age\n",
    "- x2 = occupation (1 if teacher, 0 otherwise)\n",
    "- x3 = social network size\n",
    "\n",
    "Inferring the significance of each regression coefficient works exactly the same now as linear regression:\n",
    "$$t=\\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is logistic regression not the most suitable method?\n",
    "\n",
    "- More than 2 classes\n",
    "- Parameter estimates from logistic regression become unstable when the classes are well separated.\n",
    "- If n is small and the predictors are normally distributed, parameter estimates are less stable than other methods\n",
    "\n",
    "When logistic regression can't be used due to these reasons, we can use several other methods. In the next section we'll focus on Linear discriminant analysis (LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear discriminant analysis (LDA) \n",
    "*(unsupervised method, parametric, classification)*\n",
    "\n",
    "\n",
    "**Assumptions**\n",
    "1.\tThe predictor variables are drawn from a multivariate normal distribution.\n",
    "2.\tHomogeneity of variance / covariance. This is an important simplifying assumption in LDA. \n",
    "3.\tNo multicollinearity. \n",
    "4.\tIndependence of errors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In contrast to logistic regression, which models p(X) as a function f(X), LDA models the distribution of X scores separately for each category of Y. Thus, it obtains the probability distribution of X given a specific value of Y, $p(X | Y =k)$. It then uses Bayes theorem to arrive at the desired probability, namely the probability of classifying Y to a category k given a specific value of the predictor X, $P(Y=k|X)$.\n",
    "\n",
    "- *Note:* When distributions are normal, this becomes a special case of logistic regression.\n",
    "\n",
    "Thus, LDA find the best way to carve up your data into meaningful groups. One the left figure below, we see the distribution of some predictor X, conditional on the category of Y. On the right, we see the same distribution for random samples drawn from X.\n",
    "\n",
    "![LDA](imgs/L12Fig3.png)\n",
    "\n",
    "\n",
    "*Note: [A beautiful demonstration of LDA in Python](https://sebastianraschka.com/Articles/2014_python_lda.html#normality-assumptions)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes theorem for classification:\n",
    "- Bayes theorem:\n",
    "$$ P(A|B) = \\frac{p(B|A)p(A)}{p(B)}$$\n",
    "\n",
    "    - likelihood: $p(B|A)$\n",
    "    - prior: $p(A)$\n",
    "    \n",
    "- Example: What's the probability of it having rained if the sidewalk is wet? Well, it's a function of how likely it is to rain in the first place, how likely it is to be wet if it had in fact rained, and how likely it is to be wet due to other reasons. Specifically:\n",
    "\n",
    "$$ P(rain|wet) = \\frac{p(wet|rain)p(rain)}{p(wet)}$$\n",
    "\n",
    "- As you can see, the probability of having rained given that it's wet outside:\n",
    "    - increases if you live in a rainy country (UK) and decreases if you live in Israel where it rarely rains and wetness is more likely due to other reasons\n",
    "    - decreases if there are many likely reasons of it being wet (p(wet)) - if you live in 12th century, people used to empty latrines through the window on the street, which we don't do any more\n",
    "    - increases if the rain was recent (p(wet|rain))\n",
    "\n",
    "\n",
    "- Goal: Classify any observation yiinto one of any K classes.\n",
    "- $\\pi_k$ = prior probability that a given observation is associated with the kth category\n",
    "- Frequentist vs. Bayes (point number versus probability distribution)\n",
    "- **Density function:** \n",
    "$$f_k(X) \\equiv P(X = x | Y = k)$$\n",
    "    - $f_k(X)$ is relatively large if there is a high probability that an observation in the kth class has X~= x\n",
    "- Approximation of Bayes Theorem:\n",
    "$$P(Y = K|X=X) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}$$\n",
    "- Posterior probability:\n",
    "$$ p_k(X) = P(Y = k|X) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The case with one predictor (p = 1):\n",
    "- Have to assume shape of $f_k(X)$\n",
    "- For example, Gaussian distribution:\n",
    "$$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\bigg(-\\frac{1}{2\\sigma^2_k}(x-\\mu_k)^2\\bigg)$$\n",
    "- which gives an p_k as:\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2\\bigg)}{\\sum_{l=1}^{K}\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigg)}$$\n",
    "- log(pk(x)) gives us the discriminant function:\n",
    "$$\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k)$$\n",
    "    - where $\\pi_k = \\frac{n_k}{n}$\n",
    "- **Bayes decision boundary: The point that separates the two distributions    **\n",
    "$$x = \\frac{\\mu_1^2-\\mu_2^2}{2(\\mu_1-\\mu_2)} = \\frac{\\mu_1+\\mu_2}{2}$$\n",
    "- Discriminant function: the posterior probability estimate that x is in class K.\n",
    "- LDA tries to find best parameters of k,k, and k as model parameters\n",
    "![LDA parameters](imgs/L12Fig4.png)\n",
    "    - By plugging in the parameter estimates you get the discriminant function for each k category:\n",
    "$$\\hat{\\delta}_k(x) = x\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2}-\\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2}+\\log(\\hat{\\pi}_k)$$\n",
    "    - Higher discriminant values indicate more likely x is a member of the kth group.\n",
    "- Algorithm:\n",
    "    - $Z = w_1x_1 + ... + wx_p$\n",
    "    - $S(\\beta) = (w^T\\mu_1-w^T\\mu_2)/(w^T\\sum\\beta)$\n",
    "    - Model coefficients: $w = \\sum^{-1}(\\hat{\\mu}_1-\\hat{\\mu}_2)$\n",
    "    - Pooled covariance: $\\sum = (n_1+n_2)^{-1}(n_1\\sum_1+n_2\\sum_2)$\n",
    "    - Objective function: $w^T(X-((\\hat{\\mu}_1+\\hat{\\mu}_2)/2)) > \\log(p(k = 1)/p(k = 0))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The case with multiple predictors (p > 1)\n",
    "\n",
    "It's easy to extend the LDA approach to multiple predictors. In this case, we are using the multivariate GAussian density function:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{(2\\pi)^p|\\Sigma|}}\\exp\\bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix for all K classes.\n",
    "\n",
    "The discriminat function becomes:\n",
    "$$\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+log\\pi_k$$\n",
    "\n",
    "which is just a vectorized version of the discriminant function for p=1\n",
    "\n",
    "This results in the following decision boundaries in the example figure below:\n",
    "\n",
    "![LDA decision boundaries for p=2](imgs/L12Fig5.png)\n",
    "\n",
    "- Note that there are multiple decision boundaries (one for every pair of k categories)\n",
    "- Dashed lines show real bayes decision boundaries\n",
    "- Solid lines show the LDA decision boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LDA as a multi-purpose classifier\n",
    "\n",
    "- Prediction problem: Build a model (Train), then test that model on new data (Test)\n",
    "- Beware biases:\n",
    "\n",
    "    - Overfitting: When p > n, then you will do well because you fit on noise.\n",
    "    - Proper null: E.g., in the default example, the default rate is low, so just guessing that no one will default gives you high accuracy.\n",
    "        - Null is not 50/50\n",
    "    - Consider the example in the book\n",
    "\n",
    "<img src=\"imgs/L12Fig6.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "Classifier performance evaluated against two properties:\n",
    "\n",
    "- Sensitivity: percentage of true positives identified\n",
    "- Specificity: percentage of false negatives avoided\n",
    "- Playing with decision thresholds can impact sensitivity & specificity of classifier\n",
    "\n",
    "**The ROC curve:**\n",
    "- Receiver operating characteristic. Started with radio signals\n",
    "- Plot the False alarms (x axis) against the hits (y axis)\n",
    "- Take into account all possible thresholds\n",
    "\n",
    "![ROC fig](imgs/L12ROCfig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 3. Quadratic discriminant analysis (QDA) \n",
    "*(unsupervised method, parametric, classification)*\n",
    "\n",
    "**Assumptions**\n",
    "1.\tThe predictor variables are drawn from a multivariate normal distribution.\n",
    "2.\tNo multicollinearity. \n",
    "3.\tIndependence of errors.\n",
    "\n",
    "Remember, LDA assumes a covariance matrix common to all $K$ classes. Notice that this QDA assumption is missing from above. QDA is an alternative version of LDA that does not assume homogeneity of variance. Instead, QDA assumes that each class has its own covariance matrix. In other words, the variability between classes does not have to be equal. \n",
    "Formally, it assumes that an observation $X = x$ from the $k$th class is $X âˆ¼ N(\\mu_k,\\Sigma_k)$, where $\\Sigma_k$ is a covariance matrix for the $k$th class. Then the Bayesian classifier assigns an observation for the class which maximizes: \n",
    "<img src=\"imgs/qda_discriminant.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "Unlike in LDA, the $x$ appears as a *quadratic* function in the above equation, which is where quadratic discriminant analysis gets its name.\n",
    "\n",
    "It's important to think about what this kind of flexibility means. Estimating a separate covariance matrix for each class increases the number of parameters to be estimated. The assumption of equal variance in LDA means that fewer parameters are estimated. So, LDA is more biased than QDA, and QDA is more flexible than LDA. \n",
    "\n",
    "To showcase the impact of the tradeoff between the two methods, see the figure below. This shows how both LDA and QDA perform when covariances are equal and when they are distinct. On the left, the two classes have a common covaraince. As a result, the Bayes decision boundary (purple and dashed) is linear and is well-approximated by the LDA decision boundary (black and dotted). Here, the QDA approximation of the decision boundary (green and solid) is poor, *because* of the flexibility (variance) of the estimate. On the right, the two classes have distinct variances between the $x$ variables (0.7 and -0.7). Now optimal/Bayes decision boundary is quadratic, and so QDA more accurately approximates the true boundary than LDA does, which assumes a linear boundary. \n",
    "\n",
    "<img src=\"imgs/lda_qda_bayes.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to think carefully about the tradeoff between flexibility and bias for each method, and the implications of that tradeoff for interpreting the results of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
